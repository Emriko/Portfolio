[{"content":" Zomb:E Type Description Genre First Person Shooter Engine Penugine Contributions Graphics Engine and Rendering pipeline Player Movement Combat and Gunplay Powerups VFX and correlating system Spite: Shadow of the Spirder Lily Type Description Genre Top-down action RPG Engine Penugine Contributions Graphics Engine and Rendering pipeline Combat Pathfinding Carl Off Duty Type Description Genre Top-down action adventure Engine The Game Engine (Proprietary school engine) Contributions Game Grid Player movement Combat Shadows Fallen Star Type Description Genre 2.5D Platformer Engine The Game Engine (Proprietary school engine) Contributions Collission Particle System Movement Detective Rodent Type Description Genre Mobile puzzle game Engine Unity Contributions Character controller Level interactables Planet Hopper Type Description Genre Runner Game Engine Unity Contributions Player movement Pickups Effects Collission ","date":"April 2, 2025","permalink":"/projects/","summary":" Zomb:E Type Description Genre First Person Shooter Engine Penugine Contributions Graphics Engine and Rendering pipeline Player Movement Combat and Gunplay Powerups VFX and correlating system Spite: Shadow of the Spirder Lily Type Description Genre Top-down action RPG Engine Penugine Contributions Graphics Engine and Rendering pipeline Combat Pathfinding Carl Off Duty Type Description Genre Top-down action adventure Engine The Game Engine (Proprietary school engine) Contributions Game Grid Player movement Combat Shadows Fallen Star Type Description Genre 2.5D Platformer Engine The Game Engine (Proprietary school engine) Contributions Collission Particle System Movement Detective Rodent Type Description Genre Mobile puzzle game Engine Unity Contributions Character controller Level interactables Planet Hopper Type Description Genre Runner Game Engine Unity Contributions Player movement Pickups Effects Collission ","title":"Game Projects","type":"page"},{"content":" Type Description Engine Penugine Timeline ~8 weeks 50% Your browser does not support the video tag.\rWhat this is This is a demonstration and explanation of a particle system designed as a tool for designers and procedural artists to create captivating effects for our game projects. While the primary goal was to develop this tool, a significant focus was also placed on becoming comfortable with compute shaders and techniques suited for this task.\nWith compute shaders playing an increasingly important role in offloading CPU computation and enabling effects at a scale that would be impractical with traditional CPU processing, befriending them felt core to my skill set\nImplementation Shader loading and buffer creation Start by loading our precompiled shaderobject.\nbool DX11::LoadComputeShader(const char* aPath, ID3D11ComputeShader** aComputeShader) { HRESULT result; std::ifstream csFile; csFile.open(aPath, std::ios::binary); std::string data = {std::istreambuf_iterator\u0026lt;char\u0026gt;(csFile), std::istreambuf_iterator\u0026lt;char\u0026gt;()}; result = DX11::Device-\u0026gt;CreateComputeShader(data.data(), data.size(), nullptr, aComputeShader); if (FAILED(result)) { return false; } csFile.close(); return true; } Next the buffer is created with the data of the particle struct, Amount of particles, and inital data in myParticleData.\nresult = CreateStructuredBuffer(DX11::Device, sizeof(Particle), NUM_PARTICLE, \u0026amp;myParticleData[0], myStructuredBuffer.GetAddressOf()); if (FAILED(result)) { return false; } The emmiter setup has particle data as the following.\nstruct Particle { Vector4f position = {0,0,0,0}; float lifetime = 0.f; Vector3f velocity = {0,0,0}; }; Mesh initialization For the mesh based setup, the data is would be.\nstruct Particle { Vector4f position = {0,0,0,0}; Vector4f startpositionModel1 = {0,0,0,0}; Vector4f startpositionModel2 = {0,0,0,0}; Vector4f velocity = {0,0,0,0}; Vector4f wishVelocity = {1,1,1,1}; Vector4f vColor\t= {0,0,0,0}; Vector4\u0026lt;unsigned int\u0026gt; boolData = {0,0,0,0}; }; With an accompanying example\nYour browser does not support the video tag.\rThe start position is the vertex position of the mesh. vColor is the vertex color if the model supports it, and the boolean data is represented by unsigned integers. This is because floats are 4 bytes in HLSL, whereas in C++, they are 1 byte. Preferably, this should be wrapped in a way that ensures conversion only happens when moving the data to the GPU, minimizing the possibility of user error. However, the only steps where this data is used are during the initial setup and in the HLSL code itself, so user error should not occur\nThe following video provides an example for multiple meshes\nYour browser does not support the video tag.\rFor the buffer we created, we create an unordered access view. This allows our compute shaders to read from and write to the buffer. We also create a shader resource view so that our non-compute shaders can read the data as intended. Our vertex shader will use this to output the position of each mesh representing a particle\nresult = CreateBufferUAV(DX11::Device, myStructuredBuffer.Get(), myParticleUAV.GetAddressOf()); if (FAILED(result)) { return false; } result = CreateBufferSRV(DX11::Device, myStructuredBuffer.Get(), myParticleSRV.GetAddressOf()); if (FAILED(result)) { return false; } The dispatch and render passes are as following.\nRunComputeShader(myComputeShader.Get(), 0, nullptr, nullptr, nullptr, 0, myParticleUAV.Get(), NUM_PARTICLE / 10, 10, 1); myPreprocessedFrame.SetAsActiveTarget(DX11::DepthBuffer); myTentativeState.blendState = BlendState::AlphaBlend; UpdateGpuState(); DX11::Context-\u0026gt;VSSetShaderResources(0, 1, myParticleSRV.GetAddressOf()); DX11::Context-\u0026gt;PSSetConstantBuffers(11, 1, myParticleColorBuffer.GetAddressOf()); myParticleShader-\u0026gt;Render(t); This is all that needs to be done on the CPU. myParticleShader holds the meshes for the particles themselves. In this case, they are represented by a diamond to provide some volume while keeping the geometry minimal.\nShaders The following is a simple emitter that uses pseudo-random algorithms seeded with the particle ID. This will create a repeating pattern, however, given the number of particles, this is negligible.\n[numthreads(1, 1, 1)] void main(uint3 DTid : SV_DispatchThreadID) { BufferInOut[DTid.x].lifetime.x += Timings.y; BufferInOut[DTid.x].velocity.y -= 0.00009f; if (BufferInOut[DTid.x].lifetime.x \u0026gt; 33.f) { BufferInOut[DTid.x].lifetime.x = 0.f; BufferInOut[DTid.x].velocity.xyz = up.xyz; BufferInOut[DTid.x].velocity.xyz += forward.xyz * (rand_xorshift(DTid.x) * (2.0 / 4294967296.0) - 1.f); BufferInOut[DTid.x].velocity.xyz += right.xyz * (rand_lcg(DTid.x) * (2.0 / 4294967296.0) - 1.f); BufferInOut[DTid.x].position.xyz = forcePosition; } BufferInOut[DTid.x].position.xyz += BufferInOut[DTid.x].velocity.xyz * Timings.y * 200.f; } This produces the following effect. Your browser does not support the video tag.\rThis effect uses the following vertex shader, with ObjectToWorld as an extra transform to rotate and scale the particles if so desired.\nParticlePSInput main(ParticleVSInput input) { ParticlePSInput output; float4 particlePos = Buffer0[input.id].position; float4x4 transform = float4x4( 1, 0, 0, Buffer0[input.id].position.x, 0, 1, 0, Buffer0[input.id].position.y, 0, 0, 1, Buffer0[input.id].position.z, 0, 0, 0, 1); float4x4 toNewObject = mul(transform, ObjectToWorld); float4 vertexWorldPos = mul(transform, input.position); float4 vertexViewPos = mul(WorldToCamera, vertexWorldPos); output.worldPosition = vertexWorldPos; output.position = mul(CameraToProjection, vertexViewPos); output.vertexColor0 = Buffer0[input.id].vColor; return output; } Depth collission As observed above, the particles simply fall through the ground, making them appear out of place in the 3D scene. This issue is resolved using depth collision.\nTo achieve this, we need to project the particles onto screen space to sample the depth and normal of the occupying pixels. The normal will be used upon collision for impact velocity calculations.\nIn the graphics pipeline I designed for our engine, the necessary data is stored in the geometry buffers. We bind these before the compute shader pass\nmyGbuffer.CSSetAsResourceOnSlot(GBuffer::GBufferTexture::ScreenPos, 1); myGbuffer.CSSetAsResourceOnSlot(GBuffer::GBufferTexture::Normal, 2); In the compute shader, we convert the world position into the necessary coordinate spaces. We use view space for Z-depth from our camera and projected space for screen-space coordinates to sample the pixel depth and normals\nfloat4x4 transform = float4x4( 1, 0, 0, BufferInOut[DTid.x].position.x, 0, 1, 0, BufferInOut[DTid.x].position.y, 0, 0, 1, BufferInOut[DTid.x].position.z, 0, 0, 0, 1); float4 vertexWorldPos = mul(transform, float4(0, 0, 0, 1)); float4 vertexViewPos = mul(WorldToCamera, vertexWorldPos); float4 projected = mul(CameraToProjection, vertexViewPos); We then convert the projected values into Direct3D11 projection space using the following method.\nprojected.xyz /= projected.w; projected.xy = (projected.xy + 1) * 0.5f; projected.y = 1 - projected.y; Next, we sample the projected pixel and compare it to the particle\u0026rsquo;s depth. If the particle is within a certain range of the projected depth, we apply a smear collision and add a bit of bounce, depending on the velocity and the angle at which the collision occurred.\nfloat depth = DepthTexture.SampleLevel(defaultSampler, projected.xy, 0).z; if (vertexViewPos.z \u0026lt; depth + radius \u0026amp;\u0026amp; vertexViewPos.z \u0026gt; depth - radius) { float3 normal = NormalTexture.SampleLevel(defaultSampler, projected.xy, 1).xyz * 2 - 1; if (length(BufferInOut[DTid.x].velocity.xyz) \u0026lt; 0) { return; } BufferInOut[DTid.x].velocity.xyz = BufferInOut[DTid.x].velocity.xyz - normal * dot(BufferInOut[DTid.x].velocity.xyz, normal) + normal * max(0, -dot(BufferInOut[DTid.x].velocity.xyz, normal) * length(BufferInOut[DTid.x].velocity.xyz) - 2); } This produces the following effect. Your browser does not support the video tag.\rA clear flaw with this technique is that if the object is not visible, it does not contribute to the depth buffer, and the particles do not collide. The following video demonstrates this issue.\nYour browser does not support the video tag.\rAreas of improvement Stretch goals I personally had included implementing motion blur for the particles to simulate higher velocities, making the effect feel more dynamic.\nA system like this is very flexible. New features can always be added to enable different implementations. Making the system more modular for in-game use and providing more customization options for the effects produced are also high on the priority list.\n","date":"March 26, 2025","permalink":"/posts/particlesystem/","summary":" Type Description Engine Penugine Timeline ~8 weeks 50% Your browser does not support the video tag.\r","title":"Compute Shader particle system with Depth collission","type":"posts"},{"content":" Type Description Engine Penugine Timeline ~1-2 weeks 50% Screen Space Ambient Occlusion, more commonly known as SSAO, is a post-processing technique used to approximate how occluded a pixel in a 3D scene would be. This is done to adjust the world’s ambient light accordingly, mimicking the effect of the pixel\u0026rsquo;s occlusion. This is performed as a screen space pass, taking in geometry positions and normals, hence the name Screen Space Ambient Occlusion.\nImplementation The purpose of SSAO is to determine how occluded a pixel is. To do this, we need some knowledge about the rest of the 3D scene. In our case, we take the Z-Buffer from the camera\u0026rsquo;s perspective. In this case, we save the perspective Z value in a texture before our SSAO pass. This corresponds to the depth.\nWhen determining a pixel\u0026rsquo;s occlusion, some precomputed random directions are used as an offset from the pixel\u0026rsquo;s world position, which is taken from the geometry buffer. This new position is projected into view-space to get its z-depth. With the found value, a comparison is made to the original z-depth of our pixel to determine if it\u0026rsquo;s closer to the camera. If it is, that pixel is occluded by that position, contributing to the occlusion factor. This is then repeated a handful of times to get an estimate of the amount of occlusion the pixel would have\nFigure showing occluded projected possitions in a sphere\nHowever, a pixel cannot be occluded by a position behind its own surface. To improve the efficiency of a sample, instead of using a sphere, we ensure the position is aligned with the pixel\u0026rsquo;s surface normal (see figure below).\nTo achieve this, we take the surface normal of the pixel and create a valid TBN matrix for its rotation space. A TBN matrix (Tangent, Bitangent, Normal) is used to map the random sample directions into the correct orientation in world space based on the pixel\u0026rsquo;s normal vector. This process is very uniform, so noise may be used to introduce variability, and used to rotate the TBN matrix into another valid one, resulting in less uniformity by having varied matrices.\nFigure showing hemisphere\nResults Following, a comparison image with and without the effect can be seen. The results act in a way to adding relation to each scene object and a level of depth to their surrounding backgrounds. In the comparison image, the effect of SSAO adds realistic shadows and depth to the scene, making the objects feel more grounded in the environment.\n","date":"March 26, 2025","permalink":"/posts/ssao/","summary":" Type Description Engine Penugine Timeline ~1-2 weeks 50% ","title":"Screen Space Ambient Occlusion","type":"posts"},{"content":" Type Description Engine Unity Timeline 3 days Download Link Cooking Grandma is a multiple award-winning Game Jam hosted by The Game Assembly with the theme of “Combination.” I can hands down say that the work on Cooking Grandma was the most fun I have had with a project during my stint at TGA.\nIn just a couple of days, the teamwork, creativity, and laughs really carried this project through the door. With no time to say no and just having fun, it is very impressive what can be accomplished.\nI would encourage you to download the game and try it out if you have the time!\nCredits Name Title William Arnberg Programming Gustaf Engsner Programming Emrik Åberg Wenthzel Programming Carl Anderson Graphics and vfx Hampus Helin Graphics and vfx ","date":"March 26, 2025","permalink":"/posts/cookinggrandma/","summary":" Type Description Engine Unity Timeline 3 days Download Link ","title":"Cooking Grandma 👵","type":"posts"},{"content":" Type Description Engine Penugine Timeline ~1 week 50% Purpose For a game project with the goal of mimicing CoD Zombies we needed a character controller and fluid movement. For this purpose inital system was build around the quake 2 movement for it\u0026rsquo;s plenitufl documentation and fluidity of movement. However some quirks of this system made new systems neccisary to fully reach a working in-game implementation.\nThis movement was implemented on a PhysX object. However this is a kinematic object so the motion is fully controlled by the movement controller and later on validated by PhysX.\nQuake movement Getting movemnt that feels good and intuitive to use by following in quakes footsteps is not a hard task. With this movement system being analyzed countless times, the documentation of what makes it click is mostly set in stone. With all of this and the IdTech 2 engine to refer to this was set up within the first day.\nMW3 adjustments This system, though it feels well to use, is not fully suited for our CoD zombies purpose. To fix this, studying of Call of Duty: Modern Warfare 3\u0026rsquo;s movemnt had to be done. This version was choosen due to internal cloes communities finding this to be some of the best feeling movemnt in the series. It also has a custom client where the movement can be tested in sandbox enviourments.\nBunny hopping One of the core features in the Quake IdTech movement, and the engines inspired by it is the mechanic of bunny hopping. This mechanic is based in the differance in leght of a wished velocity, current velocity, and wished direction is used to gaint more speed when the movement is turing towards a new direction. This allows for continuous increase of speed while in the air and not being effected by ground friction.\nTo keep this effect going, one jumps on the first frame the player lands to avoid triggering the ground friction. This effect is indeed very fun to use and feels quite good to preform, so why is it a problem. In a round based CoD Zombies inspired game, kiting zombies effectily is vital. Using your sorroundings to be able to keep a fair distance while not being caught offguard is a key gameplay element. So if you can just fly away at incredible speeds this becomes a bit redundant.\nSo one solution would be to add air friction. Sounds like a clear fix. One problem with this is that the movement instantly stars feeling as smooth by this, and one other key feature yet to be mentioned gets destroyed by this. That would be long jumping\nLong Jumping Long jumping is the act of almost performing a bunny hop but only once. This allows for one long fast jump by strafing just before and in the air when you jump. A core mechanic in higher levels of CoD Zombies play to get out of thight spots.\nThe fix What can be done to fix this then? Well, forcing the players to stay on the ground for a bit to force the application of ground friction would be a way. To achieve that, one implementation is to introduce jump stamina. Exactly that was done. After jumping, an internal counter goes up for how many times you jump in a row, effectivly tiring out the player if jumping too much in a row. This is then used to delaying the player from jumping again.\nPolishes Most developers know, limiting the player from doing something does not feel too well. To solve this we need to give the player the illusion of still landing while being grounded. This implementation achieved this by using the view model of the player to keep moving, scaling with the jump stamina. Making it feel more intuitive with the wait.\nWhile also handling input buffering to be able to jump without having to time the landing or waiting for the stamina was key to the fluid feeling of the movement syste.\nAreas of improvement A lot of the areas of improvment land on the systems around the movemnt. More solid sounds to acompany what you are currently doing, or clearer animations to make it feel more fluid would be on the top of the list. However these were not my areas of implementation.\nIn the system itself, most the improvemnt lay in tweaking of variables to mimic my referances more properly.\n","date":"March 26, 2025","permalink":"/posts/codzombiesmovement/","summary":" Type Description Engine Penugine Timeline ~1 week 50% ","title":"Quake inspired Movement with Cod Zombies adjustments","type":"posts"},{"content":"Gameplay and Graphics programmer\nEver since I was young, I have been very technical and loved solving problems. I usually start with the most intuitive approach and enjoy experimenting with different methods until everything eventually falls into place.\nAfter three years of computer science courses and a year-long stint at Arlanda Airport, I found myself at The Game Assembly, where I truly discovered my passion for programming while working on what I love.\nIn my free time, you’ll often find me playing video games, but beyond that, my main hobby is indoor rock climbing, hopefully soon to be outdoor climbing as well!\n","date":"March 26, 2025","permalink":"/about/","summary":"Gameplay and Graphics programmer\nEver since I was young, I have been very technical and loved solving problems. I usually start with the most intuitive approach and enjoy experimenting with different methods until everything eventually falls into place.\nAfter three years of computer science courses and a year-long stint at Arlanda Airport, I found myself at The Game Assembly, where I truly discovered my passion for programming while working on what I love.\n","title":"About me","type":"page"}]