[{"content":" Type Description Engine Penugine Timeline ~8 weeks 50% Your browser does not support the video tag.\rWhat this is This is a demonstration and explanation of a particle system designed as a tool for designers and procedural artists to create captivating effects for our game projects. While the primary goal was to develop this tool, a significant focus was also placed on becoming comfortable with compute shaders and techniques suited for this task.\nWith compute shaders playing an increasingly important role in offloading CPU computation and enabling effects at a scale that would be impractical with traditional CPU processing, befriending them felt core to my skill set\nImplementation Shader loading and buffer creation Start by loading our precompiled shaderobject.\nbool DX11::LoadComputeShader(const char* aPath, ID3D11ComputeShader** aComputeShader) { HRESULT result; std::ifstream csFile; csFile.open(aPath, std::ios::binary); std::string data = {std::istreambuf_iterator\u0026lt;char\u0026gt;(csFile), std::istreambuf_iterator\u0026lt;char\u0026gt;()}; result = DX11::Device-\u0026gt;CreateComputeShader(data.data(), data.size(), nullptr, aComputeShader); if (FAILED(result)) { return false; } csFile.close(); return true; } Next the buffer is created with the data of the particle struct, Amount of particles, and inital data in myParticleData.\nresult = CreateStructuredBuffer(DX11::Device, sizeof(Particle), NUM_PARTICLE, \u0026amp;myParticleData[0], myStructuredBuffer.GetAddressOf()); if (FAILED(result)) { return false; } The emmiter setup has particle data as the following.\nstruct Particle { Vector4f position = {0,0,0,0}; float lifetime = 0.f; Vector3f velocity = {0,0,0}; }; Mesh initialization For the mesh based setup, the data is would be.\nstruct Particle { Vector4f position = {0,0,0,0}; Vector4f startpositionModel1 = {0,0,0,0}; Vector4f startpositionModel2 = {0,0,0,0}; Vector4f velocity = {0,0,0,0}; Vector4f wishVelocity = {1,1,1,1}; Vector4f vColor\t= {0,0,0,0}; Vector4\u0026lt;unsigned int\u0026gt; boolData = {0,0,0,0}; }; With an accompanying example\nYour browser does not support the video tag.\rThe start position is the vertex position of the mesh. vColor is the vertex color if the model supports it, and the boolean data is represented by unsigned integers. This is because floats are 4 bytes in HLSL, whereas in C++, they are 1 byte. Preferably, this should be wrapped in a way that ensures conversion only happens when moving the data to the GPU, minimizing the possibility of user error. However, the only steps where this data is used are during the initial setup and in the HLSL code itself, so user error should not occur\nThe following video provides an example for multiple meshes\nYour browser does not support the video tag.\rFor the buffer we created, we create an unordered access view. This allows our compute shaders to read from and write to the buffer. We also create a shader resource view so that our non-compute shaders can read the data as intended. Our vertex shader will use this to output the position of each mesh representing a particle\nresult = CreateBufferUAV(DX11::Device, myStructuredBuffer.Get(), myParticleUAV.GetAddressOf()); if (FAILED(result)) { return false; } result = CreateBufferSRV(DX11::Device, myStructuredBuffer.Get(), myParticleSRV.GetAddressOf()); if (FAILED(result)) { return false; } The dispatch and render passes are as following.\nRunComputeShader(myComputeShader.Get(), 0, nullptr, nullptr, nullptr, 0, myParticleUAV.Get(), NUM_PARTICLE / 10, 10, 1); myPreprocessedFrame.SetAsActiveTarget(DX11::DepthBuffer); myTentativeState.blendState = BlendState::AlphaBlend; UpdateGpuState(); DX11::Context-\u0026gt;VSSetShaderResources(0, 1, myParticleSRV.GetAddressOf()); DX11::Context-\u0026gt;PSSetConstantBuffers(11, 1, myParticleColorBuffer.GetAddressOf()); myParticleShader-\u0026gt;Render(t); This is all that needs to be done on the CPU. myParticleShader holds the meshes for the particles themselves. In this case, they are represented by a diamond to provide some volume while keeping the geometry minimal.\nShaders The following is a simple emitter that uses pseudo-random algorithms seeded with the particle ID. This will create a repeating pattern, however, given the number of particles, this is negligible.\n[numthreads(1, 1, 1)] void main(uint3 DTid : SV_DispatchThreadID) { BufferInOut[DTid.x].lifetime.x += Timings.y; BufferInOut[DTid.x].velocity.y -= 0.00009f; if (BufferInOut[DTid.x].lifetime.x \u0026gt; 33.f) { BufferInOut[DTid.x].lifetime.x = 0.f; BufferInOut[DTid.x].velocity.xyz = up.xyz; BufferInOut[DTid.x].velocity.xyz += forward.xyz * (rand_xorshift(DTid.x) * (2.0 / 4294967296.0) - 1.f); BufferInOut[DTid.x].velocity.xyz += right.xyz * (rand_lcg(DTid.x) * (2.0 / 4294967296.0) - 1.f); BufferInOut[DTid.x].position.xyz = forcePosition; } BufferInOut[DTid.x].position.xyz += BufferInOut[DTid.x].velocity.xyz * Timings.y * 200.f; } This produces the following effect. Your browser does not support the video tag.\rThis effect uses the following vertex shader, with ObjectToWorld as an extra transform to rotate and scale the particles if so desired.\nParticlePSInput main(ParticleVSInput input) { ParticlePSInput output; float4 particlePos = Buffer0[input.id].position; float4x4 transform = float4x4( 1, 0, 0, Buffer0[input.id].position.x, 0, 1, 0, Buffer0[input.id].position.y, 0, 0, 1, Buffer0[input.id].position.z, 0, 0, 0, 1); float4x4 toNewObject = mul(transform, ObjectToWorld); float4 vertexWorldPos = mul(transform, input.position); float4 vertexViewPos = mul(WorldToCamera, vertexWorldPos); output.worldPosition = vertexWorldPos; output.position = mul(CameraToProjection, vertexViewPos); output.vertexColor0 = Buffer0[input.id].vColor; return output; } Depth collission As observed above, the particles just fall through the ground making otherly from the 3D scene. This issue is solved with depth collission.\nTo achieve this, we need to project the particles to screen position to be able to sample the occupying pixels depth and normal. The normal will be used uppon collission for impact velocity calculations.\nIn the graphics pipline I dessgined for our engine, we have the data we need saved in the Geometry buffers. We bind these before the compute shader pass.\nmyGbuffer.CSSetAsResourceOnSlot(GBuffer::GBufferTexture::ScreenPos, 1); myGbuffer.CSSetAsResourceOnSlot(GBuffer::GBufferTexture::Normal, 2); In the compute shader we convert the world possition to the following spaces we will need. View position for the Z-Depth from our camper, and projected for the screen space cordinates to sample the pixels depth and normals.\nfloat4x4 transform = float4x4( 1, 0, 0, BufferInOut[DTid.x].position.x, 0, 1, 0, BufferInOut[DTid.x].position.y, 0, 0, 1, BufferInOut[DTid.x].position.z, 0, 0, 0, 1); float4 vertexWorldPos = mul(transform, float4(0, 0, 0, 1)); float4 vertexViewPos = mul(WorldToCamera, vertexWorldPos); float4 projected = mul(CameraToProjection, vertexViewPos); We then convert the projected values into DirectX3D11 projectio space with the following.\nprojected.xyz /= projected.w; projected.xy = (projected.xy + 1) * 0.5f; projected.y = 1 - projected.y; Next we sample the projected pixel and comapre it to the particles depth. If the particle is whitin a certain range of the projected depth, we do smear collission and add a bit of bounce depending on the velocity and at which angle the collssion happened at.\nfloat depth = DepthTexture.SampleLevel(defaultSampler, projected.xy, 0).z; if (vertexViewPos.z \u0026lt; depth + radius \u0026amp;\u0026amp; vertexViewPos.z \u0026gt; depth - radius) { float3 normal = NormalTexture.SampleLevel(defaultSampler, projected.xy, 1).xyz * 2 - 1; if (length(BufferInOut[DTid.x].velocity.xyz) \u0026lt; 0) { return; } BufferInOut[DTid.x].velocity.xyz = BufferInOut[DTid.x].velocity.xyz - normal * dot(BufferInOut[DTid.x].velocity.xyz, normal) + normal * max(0, -dot(BufferInOut[DTid.x].velocity.xyz, normal) * length(BufferInOut[DTid.x].velocity.xyz) - 2); } This produces the following effect. Your browser does not support the video tag.\rA clear flaw with this technique is if you do not see the object. It does not provide any data to the depth buffer and the particles do not collise. The following video is an example of this issue.\nYour browser does not support the video tag.\rAreas of improvement Strech goals I peronally had were to implement motion blur to the particles, simulating higher velocities, giving the effect more life.\nA system like this is very flexible. One can always add new features to be able to achieve new implementatoins, so making this system more modular for ingame use and providing more features to customize the effects producsed are high on the priority list as well.\n","date":"March 26, 2025","permalink":"/posts/particlesystem/","summary":" Type Description Engine Penugine Timeline ~8 weeks 50% Your browser does not support the video tag.\r","title":"Compute Shader particle system with Depth collission","type":"posts"},{"content":" Type Description Engine Penugine Timeline ~1-2 weeks 50% Screen Space Ambient Occlusion, more commonly known as SSAO, is a post-processing technique used to approximate how occluded a pixel in a 3D scene would be. This is done to adjust the world’s ambient light accordingly, mimicking the effect of the pixel\u0026rsquo;s occlusion. This is performed as a screen space pass, taking in geometry positions and normals, hence the name Screen Space Ambient Occlusion.\nImplementation The purpose of SSAO is to determine how occluded a pixel is. To do this, we need some knowledge about the rest of the 3D scene. In our case, we take the Z-Buffer from the camera\u0026rsquo;s perspective. In this case, we save the perspective Z value in a texture before our SSAO pass. This corresponds to the depth.\nWhen determining a pixel\u0026rsquo;s occlusion, some precomputed random directions are used as an offset from the pixel\u0026rsquo;s world position, which is taken from the geometry buffer. This new position is projected into view-space to get its z-depth. With the found value, a comparison is made to the original z-depth of our pixel to determine if it\u0026rsquo;s closer to the camera. If it is, that pixel is occluded by that position, contributing to the occlusion factor. This is then repeated a handful of times to get an estimate of the amount of occlusion the pixel would have\nFigure showing occluded projected possitions in a sphere\nHowever, a pixel cannot be occluded by a position behind its own surface. To improve the efficiency of a sample, instead of using a sphere, we ensure the position is aligned with the pixel\u0026rsquo;s surface normal (see figure below).\nTo achieve this, we take the surface normal of the pixel and create a valid TBN matrix for its rotation space. A TBN matrix (Tangent, Bitangent, Normal) is used to map the random sample directions into the correct orientation in world space based on the pixel\u0026rsquo;s normal vector. This process is very uniform, so noise may be used to introduce variability, and used to rotate the TBN matrix into another valid one, resulting in less uniformity by having varied matrices.\nFigure showing hemisphere\nResults Following, a comparison image with and without the effect can be seen. The results act in a way to adding relation to each scene object and a level of depth to their surrounding backgrounds. In the comparison image, the effect of SSAO adds realistic shadows and depth to the scene, making the objects feel more grounded in the environment.\n","date":"March 26, 2025","permalink":"/posts/ssao/","summary":" Type Description Engine Penugine Timeline ~1-2 weeks 50% ","title":"Screen Space Ambient Occlusion","type":"posts"},{"content":" Type Description Engine Unity Timeline 3 days Download Link Cooking Grandma is a multiple award-winning Game Jam hosted by The Game Assembly with the theme of “Combination.” I can hands down say that the work on Cooking Grandma was the most fun I have had with a project during my stint at TGA.\nIn just a couple of days, the teamwork, creativity, and laughs really carried this project through the door. With no time to say no and just having fun, it is very impressive what can be accomplished.\nI would encourage you to download the game and try it out if you have the time!\nCredits Name Title William Arnber Programming Gustaf Engsner Programming Emrik Åberg Wenthzel Programming Carl Anderson Graphics and vfx Hampus Helin Graphics and vfx ","date":"March 26, 2025","permalink":"/posts/cookinggrandma/","summary":" Type Description Engine Unity Timeline 3 days Download Link ","title":"Cooking Grandma 👵","type":"posts"},{"content":"Gameplay and Graphics programmer\nEver since I was young I have been very technical and loved to solve problems. Usually starting with the most intuitive paths and love to try differnet approaches until the pieces eventualyl fall into place.\nAfter 3 years of computer science courses and a year long stint at Arlanda airpot I found myself at The Game Assembly, where I truly found my programming passion working with what i love.\nIn my freetime you will obviously be able to find me playing videogames, but beyond that my main hobby is indoors rockclimbing. (hopefully soon to be outdoors as well)\ntesting text\n","date":"March 26, 2025","permalink":"/about/","summary":"Gameplay and Graphics programmer\nEver since I was young I have been very technical and loved to solve problems. Usually starting with the most intuitive paths and love to try differnet approaches until the pieces eventualyl fall into place.\nAfter 3 years of computer science courses and a year long stint at Arlanda airpot I found myself at The Game Assembly, where I truly found my programming passion working with what i love.\n","title":"About me","type":"page"}]