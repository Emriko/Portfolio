[{"content":" Zomb-E Genre First Person Shooter Engine Penugine Contributions Graphics Engine and Rendering pipeline Player Movement Combat and Gunplay Powerups VFX and correlating system Spite: Shadow of the Spirder Lily Genre Top-down action RPG Engine Penugine Contributions Graphics Engine and Rendering pipeline Combat Pathfinding Carl Off Duty Genre Top-down action adventure Engine The Game Engine (Proprietary school engine) Contributions Game Grid Player movement Combat Shadows Fallen Star Genre 2.5D Platformer Engine The Game Engine (Proprietary school engine) Contributions Collission Particle System Movement Detective Rodent Genre Mobile puzzle game Engine Unity Contributions Character controller Level interactables Planet Hopper Genre Runner Game Engine Unity Contributions Player movement Pickups Effects Collission ","date":"April 2, 2025","permalink":"/projects/","summary":" Zomb-E Genre First Person Shooter Engine Penugine Contributions Graphics Engine and Rendering pipeline Player Movement Combat and Gunplay Powerups VFX and correlating system Spite: Shadow of the Spirder Lily Genre Top-down action RPG Engine Penugine Contributions Graphics Engine and Rendering pipeline Combat Pathfinding Carl Off Duty Genre Top-down action adventure Engine The Game Engine (Proprietary school engine) Contributions Game Grid Player movement Combat Shadows Fallen Star Genre 2.5D Platformer Engine The Game Engine (Proprietary school engine) Contributions Collission Particle System Movement Detective Rodent Genre Mobile puzzle game Engine Unity Contributions Character controller Level interactables Planet Hopper Genre Runner Game Engine Unity Contributions Player movement Pickups Effects Collission ","title":"Game Projects","type":"page"},{"content":" Type Description Engine Penugine Timeline ~8 weeks 50% Your browser does not support the video tag.\rWhat this is This is a demonstration and explanation of a particle system designed as a tool for designers and procedural artists to create captivating effects for our game projects. While the primary goal was to develop this tool, a significant focus was also placed on becoming comfortable with compute shaders and techniques suited for this task.\nWith compute shaders playing an increasingly important role in offloading CPU computation and enabling effects at a scale that would be impractical with traditional CPU processing, befriending them felt core to my skill set\nImplementation Shader loading and buffer creation Start by loading our precompiled shaderobject.\nbool DX11::LoadComputeShader(const char* aPath, ID3D11ComputeShader** aComputeShader) { HRESULT result; std::ifstream csFile; csFile.open(aPath, std::ios::binary); std::string data = {std::istreambuf_iterator\u0026lt;char\u0026gt;(csFile), std::istreambuf_iterator\u0026lt;char\u0026gt;()}; result = DX11::Device-\u0026gt;CreateComputeShader(data.data(), data.size(), nullptr, aComputeShader); if (FAILED(result)) { return false; } csFile.close(); return true; } Next the buffer is created with the data of the particle struct, Amount of particles, and inital data in myParticleData.\nresult = CreateStructuredBuffer(DX11::Device, sizeof(Particle), NUM_PARTICLE, \u0026amp;myParticleData[0], myStructuredBuffer.GetAddressOf()); if (FAILED(result)) { return false; } The emmiter setup has particle data as the following.\nstruct Particle { Vector4f position = {0,0,0,0}; float lifetime = 0.f; Vector3f velocity = {0,0,0}; }; Mesh initialization For the mesh based setup, the data is would be.\nstruct Particle { Vector4f position = {0,0,0,0}; Vector4f startpositionModel1 = {0,0,0,0}; Vector4f startpositionModel2 = {0,0,0,0}; Vector4f velocity = {0,0,0,0}; Vector4f wishVelocity = {1,1,1,1}; Vector4f vColor\t= {0,0,0,0}; Vector4\u0026lt;unsigned int\u0026gt; boolData = {0,0,0,0}; }; With an accompanying example\nYour browser does not support the video tag.\rThe start position is the vertex position of the mesh. vColor is the vertex color if the model supports it, and the boolean data is represented by unsigned integers. This is because floats are 4 bytes in HLSL, whereas in C++, they are 1 byte. Preferably, this should be wrapped in a way that ensures conversion only happens when moving the data to the GPU, minimizing the possibility of user error. However, the only steps where this data is used are during the initial setup and in the HLSL code itself, so user error should not occur\nThe following video provides an example for multiple meshes\nYour browser does not support the video tag.\rFor the buffer we created, we create an unordered access view. This allows our compute shaders to read from and write to the buffer. We also create a shader resource view so that our non-compute shaders can read the data as intended. Our vertex shader will use this to output the position of each mesh representing a particle\nresult = CreateBufferUAV(DX11::Device, myStructuredBuffer.Get(), myParticleUAV.GetAddressOf()); if (FAILED(result)) { return false; } result = CreateBufferSRV(DX11::Device, myStructuredBuffer.Get(), myParticleSRV.GetAddressOf()); if (FAILED(result)) { return false; } The dispatch and render passes are as following.\nRunComputeShader(myComputeShader.Get(), 0, nullptr, nullptr, nullptr, 0, myParticleUAV.Get(), NUM_PARTICLE / 10, 10, 1); myPreprocessedFrame.SetAsActiveTarget(DX11::DepthBuffer); myTentativeState.blendState = BlendState::AlphaBlend; UpdateGpuState(); DX11::Context-\u0026gt;VSSetShaderResources(0, 1, myParticleSRV.GetAddressOf()); DX11::Context-\u0026gt;PSSetConstantBuffers(11, 1, myParticleColorBuffer.GetAddressOf()); myParticleShader-\u0026gt;Render(t); This is all that needs to be done on the CPU. myParticleShader holds the meshes for the particles themselves. In this case, they are represented by a diamond to provide some volume while keeping the geometry minimal.\nShaders The following is a simple emitter that uses pseudo-random algorithms seeded with the particle ID. This will create a repeating pattern, however, given the number of particles, this is negligible.\n[numthreads(1, 1, 1)] void main(uint3 DTid : SV_DispatchThreadID) { BufferInOut[DTid.x].lifetime.x += Timings.y; BufferInOut[DTid.x].velocity.y -= 0.00009f; if (BufferInOut[DTid.x].lifetime.x \u0026gt; 33.f) { BufferInOut[DTid.x].lifetime.x = 0.f; BufferInOut[DTid.x].velocity.xyz = up.xyz; BufferInOut[DTid.x].velocity.xyz += forward.xyz * (rand_xorshift(DTid.x) * (2.0 / 4294967296.0) - 1.f); BufferInOut[DTid.x].velocity.xyz += right.xyz * (rand_lcg(DTid.x) * (2.0 / 4294967296.0) - 1.f); BufferInOut[DTid.x].position.xyz = forcePosition; } BufferInOut[DTid.x].position.xyz += BufferInOut[DTid.x].velocity.xyz * Timings.y * 200.f; } This produces the following effect. Your browser does not support the video tag.\rThis effect uses the following vertex shader, with ObjectToWorld as an extra transform to rotate and scale the particles if so desired.\nParticlePSInput main(ParticleVSInput input) { ParticlePSInput output; float4 particlePos = Buffer0[input.id].position; float4x4 transform = float4x4( 1, 0, 0, Buffer0[input.id].position.x, 0, 1, 0, Buffer0[input.id].position.y, 0, 0, 1, Buffer0[input.id].position.z, 0, 0, 0, 1); float4x4 toNewObject = mul(transform, ObjectToWorld); float4 vertexWorldPos = mul(transform, input.position); float4 vertexViewPos = mul(WorldToCamera, vertexWorldPos); output.worldPosition = vertexWorldPos; output.position = mul(CameraToProjection, vertexViewPos); output.vertexColor0 = Buffer0[input.id].vColor; return output; } Depth collission As observed above, the particles simply fall through the ground, making them appear out of place in the 3D scene. This issue is resolved using depth collision.\nTo achieve this, we need to project the particles onto screen space to sample the depth and normal of the occupying pixels. The normal will be used upon collision for impact velocity calculations.\nIn the graphics pipeline I designed for our engine, the necessary data is stored in the geometry buffers. We bind these before the compute shader pass\nmyGbuffer.CSSetAsResourceOnSlot(GBuffer::GBufferTexture::ScreenPos, 1); myGbuffer.CSSetAsResourceOnSlot(GBuffer::GBufferTexture::Normal, 2); In the compute shader, we convert the world position into the necessary coordinate spaces. We use view space for Z-depth from our camera and projected space for screen-space coordinates to sample the pixel depth and normals\nfloat4x4 transform = float4x4( 1, 0, 0, BufferInOut[DTid.x].position.x, 0, 1, 0, BufferInOut[DTid.x].position.y, 0, 0, 1, BufferInOut[DTid.x].position.z, 0, 0, 0, 1); float4 vertexWorldPos = mul(transform, float4(0, 0, 0, 1)); float4 vertexViewPos = mul(WorldToCamera, vertexWorldPos); float4 projected = mul(CameraToProjection, vertexViewPos); We then convert the projected values into Direct3D11 projection space using the following method.\nprojected.xyz /= projected.w; projected.xy = (projected.xy + 1) * 0.5f; projected.y = 1 - projected.y; Next, we sample the projected pixel and compare it to the particle\u0026rsquo;s depth. If the particle is within a certain range of the projected depth, we apply a smear collision and add a bit of bounce, depending on the velocity and the angle at which the collision occurred.\nfloat depth = DepthTexture.SampleLevel(defaultSampler, projected.xy, 0).z; if (vertexViewPos.z \u0026lt; depth + radius \u0026amp;\u0026amp; vertexViewPos.z \u0026gt; depth - radius) { float3 normal = NormalTexture.SampleLevel(defaultSampler, projected.xy, 1).xyz * 2 - 1; if (length(BufferInOut[DTid.x].velocity.xyz) \u0026lt; 0) { return; } BufferInOut[DTid.x].velocity.xyz = BufferInOut[DTid.x].velocity.xyz - normal * dot(BufferInOut[DTid.x].velocity.xyz, normal) + normal * max(0, -dot(BufferInOut[DTid.x].velocity.xyz, normal) * length(BufferInOut[DTid.x].velocity.xyz) - 2); } This produces the following effect. Your browser does not support the video tag.\rA clear flaw with this technique is that if the object is not visible, it does not contribute to the depth buffer, and the particles do not collide. The following video demonstrates this issue.\nYour browser does not support the video tag.\rAreas of improvement Stretch goals I personally had included implementing motion blur for the particles to simulate higher velocities, making the effect feel more dynamic.\nA system like this is very flexible. New features can always be added to enable different implementations. Making the system more modular for in-game use and providing more customization options for the effects produced are also high on the priority list.\n","date":"March 26, 2025","permalink":"/posts/particlesystem/","summary":" Type Description Engine Penugine Timeline ~8 weeks 50% Your browser does not support the video tag.\r","title":"Compute Shader particle system with Depth collission","type":"posts"},{"content":" Type Description Engine Penugine Timeline ~1-2 weeks 50% Screen Space Ambient Occlusion, more commonly known as SSAO, is a post-processing technique used to approximate how occluded a pixel in a 3D scene would be. This is done to adjust the world’s ambient light accordingly, mimicking the effect of the pixel\u0026rsquo;s occlusion. This is performed as a screen space pass, taking in geometry positions and normals, hence the name Screen Space Ambient Occlusion.\nImplementation The purpose of SSAO is to determine how occluded a pixel is. To do this, we need some knowledge about the rest of the 3D scene. In our case, we take the Z-Buffer from the camera\u0026rsquo;s perspective. In this case, we save the perspective Z value in a texture before our SSAO pass. This corresponds to the depth.\nWhen determining a pixel\u0026rsquo;s occlusion, some precomputed random directions are used as an offset from the pixel\u0026rsquo;s world position, which is taken from the geometry buffer. This new position is projected into view-space to get its z-depth. With the found value, a comparison is made to the original z-depth of our pixel to determine if it\u0026rsquo;s closer to the camera. If it is, that pixel is occluded by that position, contributing to the occlusion factor. This is then repeated a handful of times to get an estimate of the amount of occlusion the pixel would have\nFigure showing occluded projected possitions in a sphere\nHowever, a pixel cannot be occluded by a position behind its own surface. To improve the efficiency of a sample, instead of using a sphere, we ensure the position is aligned with the pixel\u0026rsquo;s surface normal (see figure below).\nTo achieve this, we take the surface normal of the pixel and create a valid TBN matrix for its rotation space. A TBN matrix (Tangent, Bitangent, Normal) is used to map the random sample directions into the correct orientation in world space based on the pixel\u0026rsquo;s normal vector. This process is very uniform, so noise may be used to introduce variability, and used to rotate the TBN matrix into another valid one, resulting in less uniformity by having varied matrices.\nFigure showing hemisphere\nResults Following, a comparison image with and without the effect can be seen. The results act in a way to adding relation to each scene object and a level of depth to their surrounding backgrounds. In the comparison image, the effect of SSAO adds realistic shadows and depth to the scene, making the objects feel more grounded in the environment.\n","date":"March 26, 2025","permalink":"/posts/ssao/","summary":" Type Description Engine Penugine Timeline ~1-2 weeks 50% ","title":"Screen Space Ambient Occlusion","type":"posts"},{"content":" Type Description Engine Penugine Timeline ~1 week 50% Your browser does not support the video tag.\rPurpose For a game project aiming to mimic CoD Zombies, we needed a character controller and fluid movement. The initial system was built around Quake 2 movement due to its plentiful documentation and fluidity. However, some quirks of this system necessitated additional modifications to achieve a fully functional in-game implementation.\nThis movement was implemented on a PhysX object. However, since this is a kinematic object, motion is fully controlled by the movement controller and later validated by PhysX.\nQuake movement Creating movement that feels good and intuitive by following in Quake’s footsteps is not a difficult task. This movement system has been analyzed countless times, and the documentation detailing what makes it work is well established. With these resources and the IdTech 2 engine as a reference, the initial setup was completed within the first day.\nMW3 adjustments While this system feels great to use, it is not entirely suited for our CoD Zombies-inspired gameplay. To address this, we studied the movement mechanics of Call of Duty: Modern Warfare 3. This version was chosen due to internal close communities considering it one of the best-feeling movement systems in the series. Additionally, MW3 has a custom client where movement can be tested in sandbox environments.\nBunny hopping Your browser does not support the video tag.\rOne of the core features of Quake\u0026rsquo;s IdTech movement, and the engines inspired by it, is bunny hopping. This mechanic relies on the difference in length between the desired velocity, current velocity, and desired direction to gain additional speed when changing direction mid-air. This allows for a continuous increase in speed while airborne, avoiding the effects of ground friction.\nTo maintain this effect, players must jump on the first frame upon landing to prevent triggering ground friction. While this mechanic is fun and feels great to perform, it poses a problem in a round-based CoD Zombies-style game. Effective kiting of zombies is a vital gameplay element, requiring players to use their surroundings to maintain distance without being caught off guard. If players can continuously gain speed and evade threats effortlessly, the challenge of the game is diminished.\nA potential solution is adding air friction. While this might seem like a clear fix, it significantly reduces the smoothness of movement and interferes with another key feature: long jumping.\nLong Jumping Your browser does not support the video tag.\rLong jumping involves executing a single extended jump by strafing just before and during takeoff. This technique is crucial in high-level CoD Zombies play for escaping tight situations quickly.\nThe fix How can we balance these mechanics? One solution is to enforce brief ground contact to apply ground friction. This was implemented by introducing a jump stamina system. Each consecutive jump increases an internal counter, effectively tiring out the player if they jump too frequently. This mechanic introduces a delay before the player can jump again, preventing excessive bunny hopping while preserving movement fluidity.\nPolishes As most developers know, restricting player movement often leads to frustration. To counteract this, we needed to create the illusion of grounded movement even while enforcing the jump delay. This was achieved by adjusting the player\u0026rsquo;s view model to maintain motion, scaling with the jump stamina to make the wait feel more natural.\nAdditionally, input buffering was implemented, allowing players to jump smoothly without needing to precisely time their landings or wait for stamina recovery. This was key to maintaining the fluid feel of the movement system.\nAreas of improvement Many areas for improvement lie in the surrounding systems rather than the movement itself. More refined sound design to reflect player actions and clearer animations to enhance fluidity would significantly improve the experience. However, thses were not my areas of responsibility.\nWithin the movement system itself, most improvements would involve fine-tuning variables to better replicate the reference movement mechanics.\n","date":"March 26, 2025","permalink":"/posts/codzombiesmovement/","summary":" Type Description Engine Penugine Timeline ~1 week 50% Your browser does not support the video tag.\r","title":"Quake inspired Movement with Cod Zombies adjustments","type":"posts"},{"content":" Type Description Engine Unity Timeline 3 days Download Link Cooking Grandma is a multiple award-winning Game Jam hosted by The Game Assembly with the theme of “Combination.” I can hands down say that the work on Cooking Grandma was the most fun I have had with a project during my stint at TGA.\nIn just a couple of days, the teamwork, creativity, and laughs really carried this project through the door. With no time to say no and just having fun, it is very impressive what can be accomplished.\nI would encourage you to download the game and try it out if you have the time!\nCredits Name Title William Arnberg Programming Gustaf Engsner Programming Emrik Åberg Wenthzel Programming Carl Anderson Graphics and vfx Hampus Helin Graphics and vfx ","date":"March 26, 2025","permalink":"/posts/cookinggrandma/","summary":" Type Description Engine Unity Timeline 3 days Download Link ","title":"Cooking Grandma 👵","type":"posts"},{"content":"Gameplay and Graphics programmer\nEver since I was young, I have been very technical and loved solving problems. I usually start with the most intuitive approach and enjoy experimenting with different methods until everything eventually falls into place.\nAfter three years of computer science courses and a year-long stint at Arlanda Airport, I found myself at The Game Assembly, where I truly discovered my passion for programming while working on what I love.\nIn my free time, you’ll often find me playing video games, but beyond that, my main hobby is indoor rock climbing, hopefully soon to be outdoor climbing as well!\n","date":"March 26, 2025","permalink":"/about/","summary":"Gameplay and Graphics programmer\nEver since I was young, I have been very technical and loved solving problems. I usually start with the most intuitive approach and enjoy experimenting with different methods until everything eventually falls into place.\nAfter three years of computer science courses and a year-long stint at Arlanda Airport, I found myself at The Game Assembly, where I truly discovered my passion for programming while working on what I love.\n","title":"About me","type":"page"}]